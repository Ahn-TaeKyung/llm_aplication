{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "511281f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello LangChain\n"
     ]
    }
   ],
   "source": [
    "print('Hello LangChain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "901088f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dcf8d043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 개발자입니다.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\", \"당신은 개발자입니다.\") , \n",
    "     (\"user\", \"{input}\") ]\n",
    ")\n",
    "print(prompt)\n",
    "\n",
    "prompt_text = prompt.format(input=\"Langserve는 무엇인가요? 자세하게 설명해주세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "257df4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000002273508E840> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000022735089BB0> root_client=<openai.OpenAI object at 0x00000227350A33B0> root_async_client=<openai.AsyncOpenAI object at 0x000002273508DA00> model_name='meta-llama/llama-4-scout-17b-16e-instruct' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "# Groq API를 사용하는 ChatOpenAI 인스턴스 생성\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    # model=\"mistral-saba-24b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fbf104c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "<class 'str'>\n",
      "응답: Langserve는 개발자가 대규모 언어 모델(LLM)을 쉽게 배포하고 관리할 수 있도록 지원하는 플랫폼입니다. Langserve는 언어 모델의 개발, 테스트, 배포 및 운영을 간소화하여 개발자가 더 효율적으로 작업할 수 있도록 돕습니다.\n",
      "\n",
      "Langserve의 주요 기능은 다음과 같습니다:\n",
      "\n",
      "1. **언어 모델 관리**: Langserve는 다양한 언어 모델을 지원하며, 개발자가 자신의 모델을 쉽게 등록하고 관리할 수 있도록 합니다. 이를 통해 개발자는 여러 모델을 한 곳에서 관리할 수 있어 효율성이 높아집니다.\n",
      "\n",
      "2. **모델 배포**: Langserve는 언어 모델을 쉽고 빠르게 배포할 수 있는 기능을 제공합니다. 개발자는 Langserve를 통해 모델을 클라우드 환경에 배포할 수 있으며, 이를 통해 모델의 확장성과 가용성을 높일 수 있습니다.\n",
      "\n",
      "3. **API 제공**: Langserve는 언어 모델을 위한 API를 제공하여 개발자가 모델을 다른 애플리케이션에 쉽게 통합할 수 있도록 합니다. 이를 통해 개발자는 별도의 API 개발 없이도 언어 모델을 활용할 수 있습니다.\n",
      "\n",
      "4. **모니터링 및 분석**: Langserve는 배포된 모델의 성능을 실시간으로 모니터링하고 분석할 수 있는 기능을 제공합니다. 이를 통해 개발자는 모델의 성능을 지속적으로 평가하고 필요한 경우 모델을 업데이트할 수 있습니다.\n",
      "\n",
      "5. **커뮤니티 지원**: Langserve는 개발자들이 서로의 경험을 공유하고 협력할 수 있는 커뮤니티를 제공합니다. 이를 통해 개발자들은 서로의 지식을 공유하고 빠르게 성장할 수 있습니다.\n",
      "\n",
      "Langserve는 대규모 언어 모델을 활용하는 개발자에게 매우 유용한 도구입니다. Langserve를 사용하면 개발자는 언어 모델의 개발, 배포 및 관리에 드는 시간을 줄이고, 모델의 성능을 높이는 데 집중할 수 있습니다. 또한 Langserve는 계속해서 새로운 기능과 업데이트를 제공하며, 개발자들의 피드백을 기반으로 지속적인 개선을 이루어내고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = llm.invoke(prompt_text)\n",
    "    print(type(response))\n",
    "    print(type(response.content))\n",
    "    print(\"응답:\", response.content)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc00d1a4",
   "metadata": {},
   "source": [
    "### LCEL\n",
    "- Prompt + LLM을 Chain으로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "964a983e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='\\n    You are an expert in AI Expert. Answer the question. \\n    <Question>: {input}에 대해 쉽게 반드시 한글로 설명해주세요.\")\\n    ')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an expert in AI Expert. Answer the question. \n",
    "    <Question>: {input}에 대해 쉽게 반드시 한글로 설명해주세요.\")\n",
    "    \"\"\")                                     \n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d5ccad74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "# chain 연결 (LCEL) prompt + llm 연결\n",
    "chain = prompt | llm\n",
    "print(type(chain))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d98d0da",
   "metadata": {},
   "source": [
    "### LCEL\n",
    "- Prompt + LLM + OutputParser을 Chain으로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d02757ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# chain 연결 (LCEL) prompt + llm + outputparser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain2 = prompt | llm | output_parser\n",
    "print(type(chain2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5505446c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "인공지능 모델의 학습 원리는 컴퓨터가 데이터를 통해 스스로 학습하고, 그 학습을 통해 특정 작업을 수행하는 능력을 키우는 과정입니다. 이를 쉽게 설명해 드리겠습니다.\n",
      "\n",
      "### 1. 데이터 수집\n",
      "인공지능 모델이 학습하기 위해서는 많은 양의 데이터가 필요합니다. 이 데이터는 문제에 따라 달라지는데, 예를 들어 이미지 인식 모델을 만든다면 수백만 장의 이미지를 수집해야 합니다.\n",
      "\n",
      "### 2. 데이터 준비\n",
      "수집한 데이터를 모델이 학습할 수 있도록 준비합니다. 여기에는 데이터의 라벨링(레이블링)이 포함됩니다. 라벨링은 데이터에 정답을 달아주는 과정입니다. 예를 들어, 고양이 이미지에는 '고양이'라는 라벨을, 강아지 이미지에는 '강아지'라는 라벨을 붙입니다.\n",
      "\n",
      "### 3. 모델 선택\n",
      "어떤 유형의 인공지능 모델을 사용할지 선택합니다. 예를 들어, 이미지 인식에는 합성곱 신경망(CNN), 자연어 처리에는 순환 신경망(RNN) 또는 트랜스포머 모델을 사용할 수 있습니다.\n",
      "\n",
      "### 4. 학습 과정\n",
      "- **신경망 구성**: 선택한 모델에 따라 여러 층(layer)으로 구성된 신경망을 만듭니다. 각 층은 입력 데이터에서 특징을 추출하는 역할을 합니다.\n",
      "- **전방 전달(Forward Pass)**: 입력 데이터를 신경망에 넣고, 각 층에서 계산을 수행하여 결과를 출력합니다.\n",
      "- **오차 계산**: 출력 결과와 실제 정답(라벨) 간의 오차를 계산합니다. 이 오차를 최소화하는 것이 목표입니다.\n",
      "- **역전파(Backpropagation)**: 오차를 역으로 추적하며 각 층의 가중치를 어떻게 업데이트할지 계산합니다. 이 과정을 통해 모델은 학습합니다.\n",
      "- **가중치 업데이트**: 계산된 오차를 바탕으로 신경망의 가중치를 조정합니다. 이 과정은 최적화 알고리즘(예: SGD, Adam)에 의해 수행됩니다.\n",
      "\n",
      "### 5. 반복 학습\n",
      "전방 전달, 오차 계산, 역전파, 가중치 업데이트 과정을 여러 번 반복합니다. 반복할수록 모델의 성능이 개선되며, 더 정확한 예측을 할 수 있게 됩니다.\n",
      "\n",
      "### 6. 평가\n",
      "학습이 완료된 후, 테스트 데이터를 사용하여 모델의 성능을 평가합니다. 이 단계에서 모델의 정확도, 정밀도, 재현율 등의 지표를 계산합니다.\n",
      "\n",
      "### 7. 미세 조정\n",
      "평가 결과에 따라 모델의 성능을 더 개선하기 위해 하이퍼파라미터를 조정하거나, 더 많은 데이터를 수집하여 학습을 추가로 진행할 수 있습니다.\n",
      "\n",
      "이처럼 인공지능 모델은 데이터와 반복적인 학습을 통해 스스로를 개선하며, 주어진 작업을 수행하는 능력을 키웁니다.\n"
     ]
    }
   ],
   "source": [
    "# chain 호출\n",
    "try:\n",
    "    result = chain.invoke({\"input\": \"인공지능 모델의 학습 원리\"})\n",
    "    print(type(result))\n",
    "    print(result.content)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a835b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 호출\n",
    "try:\n",
    "    result = chain2.invoke({\"input\": \": LangChain의 Products(제품)는 어떤 것들이 있나요? 예를 들어 LangSmith, LangServe 같은 Product가 있어\"})\n",
    "    print(type(result))\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc34c496",
   "metadata": {},
   "source": [
    "### Runnable의 stream() 함수 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae53134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍 출력을 위한 요청\n",
    "try:\n",
    "    answer = chain2.stream({\"input\": \"인공지능 모델의 학습 원리를 자세하게 설명해 주세요.\"})\n",
    "    # 스트리밍 출력 \n",
    "    print(answer)\n",
    "    \n",
    "    for token in answer:\n",
    "        # 스트림에서 받은 데이터의 내용을 출력합니다. 줄바꿈 없이 이어서 출력하고, 버퍼를 즉시 비웁니다.\n",
    "        print(token, end=\"\", flush=True)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1b9ace",
   "metadata": {},
   "source": [
    "### Multi Chain\n",
    "\n",
    "- 첫번째 Chain의 출력이, 두번째 Chain의 입력이 된다.\n",
    "- 두개의 Chain과 Prompt + OutputParser를 LCEL로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bae55612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: 사용자가 입력한 장르에 따라 영화 추천\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{genre} 장르에서 추천할 만한 한국 영화를 한 편 알려주세요.\")\n",
    "\n",
    "# Step 2: 추천된 영화의 줄거리를 요약\n",
    "prompt2 = ChatPromptTemplate.from_template(\"{movie} 추전한 영화의 제목을 먼저 알려주시고, 줄을 바꾸어서 영화의 줄거리를 10문장으로 요약해 주세요.\")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 체인 1: 영화 추천 (입력: 장르 → 출력: 영화\n",
    "chain1 = prompt1 | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c08860",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 체인 2: 줄거리 요약 (입력: 영화 제목 → 출력: 줄거리)\n",
    "try:\n",
    "    chain2 = (\n",
    "        {\"movie\": chain1}  # chain1의 출력을 movie 입력 변수로 전달\n",
    "        | prompt2\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 실행: \"SF\" 장르의 영화 추천 및 줄거리 요약\n",
    "    response = chain2.invoke({\"genre\": \"액션\"})\n",
    "    print(response)  \n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d4c2cd",
   "metadata": {},
   "source": [
    "### PromptTemplate 여러개 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6fed7c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT 모델의 학습 원리는 다음과 같습니다.\n",
      "\n",
      "ChatGPT는 대규모 언어 모델로, 수십억 개의 매개변수를 가지고 있습니다. 이 모델은 인터넷에서 수집한 대규모 텍스트 데이터를 기반으로 학습하여 언어 패턴과 구조를 학습합니다. 학습 과정에서 모델은 주어진 입력에 대해 다음에 올 단어를 예측하도록 훈련되며, 이를 통해 자연스러운 텍스트를 생성할 수 있습니다.\n",
      "\n",
      "ChatGPT 모델의 장점은 다음과 같습니다.\n",
      "\n",
      "* 자연스러운 텍스트 생성: ChatGPT는 학습 데이터를 기반으로 자연스러운 텍스트를 생성할 수 있습니다.\n",
      "* 다양한 주제 이해: ChatGPT는 다양한 주제에 대해 이해하고 답변할 수 있습니다.\n",
      "* 대화형 인터페이스: ChatGPT는 대화형 인터페이스를 제공하여 사용자가 질문하고 답변을 받을 수 있습니다.\n",
      "\n",
      "ChatGPT 모델과 비슷한 AI 모델은 다음과 있습니다.\n",
      "\n",
      "* LLaMA\n",
      "* PaLM\n",
      "* BERT\n",
      "* RoBERTa\n",
      "* transformer-XL\n",
      "\n",
      "이러한 모델들은 모두 대규모 언어 모델로, 자연어 처리 및 생성 작업에 사용됩니다.\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 요약해서 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# 템플릿에 값을 채워서 프롬프트를 완성\n",
    "filled_prompt = prompt_template.format(model_name=\"ChatGPT\", count=3)\n",
    "\n",
    "# 문자열 템플릿 결합 (PromptTemplate + PromptTemplate + 문자열)\n",
    "combined_prompt = (\n",
    "              prompt_template\n",
    "              + PromptTemplate.from_template(\"\\n\\n 그리고 {model_name} 모델의 장점을 요약 정리해 주세요\")\n",
    "              + \"\\n\\n {model_name} 모델과 비슷한 AI 모델은 어떤 것이 있나요? 모델명은 {language}로 답변해 주세요.\"\n",
    ")\n",
    "combined_prompt.format(model_name=\"ChatGPT\", count=3, language=\"영어\")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "chain = combined_prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"model_name\":\"ChatGPT\", \"count\":3, \"language\":\"영어\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "436d229d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT-4 모델의 학습 원리를 2 문장으로 요약해서 한국어로 답변해 주세요.', 'Gemma 모델의 학습 원리를 3 문장으로 요약해서 한국어로 답변해 주세요.', 'llama-4 모델의 학습 원리를 4 문장으로 요약해서 한국어로 답변해 주세요.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 요약해서 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "questions = [\n",
    "    {\"model_name\": \"GPT-4\", \"count\": 2},\n",
    "    {\"model_name\": \"Gemma\", \"count\": 3},\n",
    "    {\"model_name\": \"llama-4\", \"count\": 4},\n",
    "]\n",
    "\n",
    "# 여러 개의 프롬프트를 미리 생성\n",
    "formatted_prompts = [prompt_template.format(**q) for q in questions]\n",
    "print(formatted_prompts)  # 미리 생성된 질문 목록 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1a0aade3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4 모델은 대규모의 텍스트 데이터를 학습하여 언어 패턴과 관계를 파악하고, 이를 기반으로 자연어 처리 작업을 수행하는 인공지능 모델입니다. 이 모델은 강화 학습 및 지도 학습을 사용하여 기존 모델의 성능을 크게 개선했으며 수십억 개의 매개변수를 조정하여 복잡한 언어 작업을 수행할 수 있습니다.\n",
      "Gemma는 컴퓨터가 자연어로 처리 및 생성 작업을 더 잘 수행하도록 설계된 경량 언어 모델입니다. 대규모 언어 모델과 유사한 아키텍처를 공유하지만 더 적은 매개변수로 학습되어 효율성과 접근성이 뛰어납니다. Gemma는 다양한 자연어 처리 작업에 사용할 수 있는 사전 학습된 모델을 제공하여 언어 이해 및 생성을 개선합니다.\n",
      "llama-4 모델은 메타에서 개발한 대규모 언어 모델입니다. 이 모델은 방대한 텍스트 데이터를 기반으로 학습되며, 이를 통해 자연어 처리 능력을 습득합니다. 학습 과정에서 모델은 입력된 텍스트의 패턴과 구조를 분석하고, 이를 바탕으로 다음에 나타날 단어 또는 문장을 예측합니다. 이 과정을 반복하면서 모델은 언어에 대한 이해와 생성 능력을 향상시키게 됩니다.\n"
     ]
    }
   ],
   "source": [
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "for prompt in formatted_prompts:\n",
    "    response = llm.invoke(prompt) #AIMessage\n",
    "    print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
